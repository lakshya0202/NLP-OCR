{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lakshyajagadish/opt/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 470610)\n",
      "(13, 470610)\n",
      "Bigram Tf-Idf with tokenizer\n",
      "Train score: 1.0 ; Validation score: 0.846\n",
      "\n",
      "['negative' 'negative' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative']\n",
      "Bigram Tf-Idf without tokenizer\n",
      "Train score: 1.0 ; Validation score: 0.8596\n",
      "\n",
      "['negative' 'negative' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def buildTrainingDataSet():\n",
    "    trainingData=[]\n",
    "    sentiment=[]\n",
    "    with open(\"imdb_small.csv\", mode='rt') as datafile:\n",
    "        data=csv.reader(datafile,delimiter=',',quotechar='\\\"')\n",
    "        count=0\n",
    "        for row in data:\n",
    "            if count!=0:\n",
    "                trainingData.append(row[0])\n",
    "                sentiment.append(row[1])\n",
    "            count+=1\n",
    "    return (trainingData, sentiment)\n",
    "    \n",
    "\n",
    "class PreProcessTrainingData:\n",
    "    def preProcess(self, trainingData):\n",
    "        preProcess=[]\n",
    "        for review in trainingData:\n",
    "            preProcess.append(self.process(review))\n",
    "        return preProcess\n",
    "    \n",
    "    def process(self, review):\n",
    "        review=review.lower()\n",
    "        review=re.sub('[\\!]+',\" exclaim\",review)\n",
    "        review=re.sub(r'(.)\\1+', r'\\1', review)\n",
    "        return review\n",
    "\n",
    "class lemmatize_tokenize:\n",
    "    def __init__(self):\n",
    "        self.punct=list(punctuation)\n",
    "        self.stwrds= set(stopwords.words('english')+self.punct+[\"<br />\"]+[''])\n",
    "        self.porter = PorterStemmer()\n",
    "    \n",
    "    def __call__(self, review):\n",
    "        review=word_tokenize(review)\n",
    "        review=[WordNetLemmatizer().lemmatize(word) for word in review]\n",
    "        review = [self.porter.stem(word) for word in review]\n",
    "        return [word for word in review if word not in self.stwrds and len(word)>3]\n",
    "    \n",
    "\n",
    "def Bigram_Tokenizer(processedData, processedTestData):\n",
    "    # Bigram Counts\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),tokenizer=lemmatize_tokenize())\n",
    "    bigram_vectorizer.fit(processedData)\n",
    "    X_train_bigram = bigram_vectorizer.transform(processedData)\n",
    "    X_test_bigram = bigram_vectorizer.transform(processedTestData)\n",
    "    # Bigram Tf-Idf\n",
    "    bigram_tf_idf_transformer = TfidfTransformer()\n",
    "    bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "    X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "    X_test_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_test_bigram)\n",
    "    return X_train_bigram_tf_idf,X_test_bigram_tf_idf\n",
    "\n",
    "\n",
    "def Bigram(processedData, processedTestData):\n",
    "    # Bigram Counts\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    bigram_vectorizer.fit(processedData)\n",
    "    X_train_bigram = bigram_vectorizer.transform(processedData)\n",
    "    X_test_bigram = bigram_vectorizer.transform(processedTestData)\n",
    "    # Bigram Tf-Idf\n",
    "    bigram_tf_idf_transformer = TfidfTransformer()\n",
    "    bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "    X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "    X_test_bigram_tf_idf= bigram_tf_idf_transformer.transform(X_test_bigram)\n",
    "    return X_train_bigram_tf_idf, X_test_bigram_tf_idf\n",
    "\n",
    "def train_test_scores(X: csr_matrix, Y: np.array,test:csr_matrix, title: str) -> None:\n",
    "    clf = SGDClassifier()\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, Y, train_size=0.50, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 4)} ; Validation score: {round(valid_score, 4)}\\n')\n",
    "    clf.fit(X,Y)\n",
    "    print(clf.predict(test))\n",
    "    \n",
    "trainingData, sentiment=buildTrainingDataSet()\n",
    "PrePro=PreProcessTrainingData()\n",
    "processedData=PrePro.preProcess(trainingData)\n",
    "\n",
    "\n",
    "\n",
    "test=[]\n",
    "with open('test.csv', mode='rt') as testfile:\n",
    "    testcase=csv.reader(testfile)\n",
    "    for row in testcase:\n",
    "        test.append(str(row))\n",
    "\n",
    "processedTestData=PrePro.preProcess(test)\n",
    "train_tok, test_tok=Bigram_Tokenizer(processedData,processedTestData)\n",
    "train_norm, test_norm=Bigram(processedData,processedTestData)\n",
    "print(train_norm.shape)\n",
    "print(test_norm.shape)\n",
    "train_test_scores(train_tok, sentiment, test_tok, 'Bigram Tf-Idf with tokenizer')\n",
    "train_test_scores(train_norm, sentiment,test_norm ,'Bigram Tf-Idf without tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
